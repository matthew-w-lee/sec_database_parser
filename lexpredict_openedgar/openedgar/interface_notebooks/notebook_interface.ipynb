{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JUPYTER NOTEBOOK INTERFACE TO SEC FILING DATABASE\n",
    "\n",
    "# The first part of this notebook provides a client to the Django database as a REST API. \n",
    "# The client allows for the searching of the tables of financial data contained in a SEC filing\n",
    "\n",
    "# The second part of this notebook provides a parser that processes and cleans the found table data \n",
    "# into a more useful format\n",
    "\n",
    "\n",
    "# PART 1:\n",
    "\n",
    "# The API provides 3 endpoints with the following purposes: \n",
    "# (i) getting a list of companies in database,\n",
    "# (ii) getting a list of filings for a certain company in the database by CIK (unique id number of a company \n",
    "#      used by the SEC), and \n",
    "# (iii) searching the tables of financial data in a filing; search can be performed for a financial statement type\n",
    "#       or for certain terms.\n",
    " \n",
    "import pandas\n",
    "import numpy\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Set pandas display options to improve visibility of data\n",
    "pandas.set_option(\"display.max_colwidth\", -1)\n",
    "pandas.set_option(\"display.width\", 1000)\n",
    "pandas.set_option(\"display.max_rows\", 1000)\n",
    "pandas.set_option(\"display.max_columns\", 1000)\n",
    "\n",
    "class DatabaseAPIClient():\n",
    "    \n",
    "    BASE_API_URL = \"http://openedgar:8000/api/companies/\"\n",
    "    \n",
    "    # List all companies in database\n",
    "    # Returns: dict w/ keys \"date\", \"cik\", \"name\", \"sic\"\n",
    "    def get_companies(self):\n",
    "        response = requests.get(self.BASE_API_URL).text\n",
    "        return json.loads(response)\n",
    "    \n",
    "    # List all filings for a single company\n",
    "    # Returns Dict with keys: \"id\", \"accession_number\", \"date_filed\", \"form_type\", \"is_error\", \"is_processed\", \"notes\"    \n",
    "    def get_filings(self, cik):\n",
    "        response = requests.get(self.BASE_API_URL + cik).text\n",
    "        return json.loads(response)\n",
    "\n",
    "    # Search tables in a single filing\n",
    "    # Key parameter: the json_request_data argument identifies the type of search to perform through the key \"request_type\"\n",
    "    # Returns: \n",
    "    def search_single_filing(self, cik, filing_pk, json_request_data):\n",
    "        response = requests.get(\"{}{}/filing/{}\".format(self.BASE_API_URL, cik, filing_pk), json=json_request_data).text\n",
    "        if response:\n",
    "            return json.loads(response)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    # Displays results of get_companies\n",
    "    def display_companies(self):\n",
    "        companies = self.get_companies()\n",
    "        display(pandas.DataFrame(companies))\n",
    "\n",
    "    # Displays results of get_filings\n",
    "    def display_company_filings(self, cik):\n",
    "        filings = self.get_filings(cik)\n",
    "        df = pandas.DataFrame(filings)\n",
    "        df.index = df[\"id\"]\n",
    "        display(df)\n",
    "\n",
    "    # Iterates over all of a company's filings and displays search results\n",
    "    def display_all_search_results(self, cik, json_request_data):\n",
    "        filings = self.get_filings(cik)\n",
    "        for result_index, filing in enumerate(filings):\n",
    "            print(\"id: {0}; accession number: {1}\".format(filing['id'], filing['accession_number']))\n",
    "            response = self.search_single_filing(cik, filing[\"id\"], json_request_data)\n",
    "            if response:\n",
    "                for r in response['results']:\n",
    "                    display(pandas.DataFrame(r['table_data']))\n",
    "            else:\n",
    "                print(\"NONE FOUND\")\n",
    "\n",
    "    def search_results(self, cik, json_request_data):\n",
    "        results = []\n",
    "        filings = self.get_filings(cik)\n",
    "        for result_index, filing in enumerate(filings):\n",
    "            print(\"processing id: {0}; accession number: {1}\".format(filing['id'], filing['accession_number']))\n",
    "            response = self.search_single_filing(cik, filing[\"id\"], json_request_data)\n",
    "            if response:\n",
    "                for r in response['results']:\n",
    "                    results.append(r['table_data'])\n",
    "        return results\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell can be used to execute commands\n",
    "\n",
    "client = DatabaseAPIClient()\n",
    "\n",
    "# The CIK identification number for the company to be searched in database\n",
    "cik = \"910329\"\n",
    "\n",
    "# The primary key for the filing to be searched in database\n",
    "filing_pk = 590\n",
    "\n",
    "# Sets forth options for the searching of tables in filings\n",
    "json_request_data = {\n",
    "    # Type of table to be searched in filing\n",
    "    \"request_type\": \"financial_statements\",\n",
    "    \n",
    "    # If request type is financial statement, type of financial statements requested\n",
    "    \"statement_type\": \"balance_sheet\",\n",
    "\n",
    "    # If request type is search, terms to search in tables of filing\n",
    "    \"search_terms\": [\"term1\", \"term2\", \"term3\"],\n",
    "}\n",
    "\n",
    "\n",
    "#client.display_companies()\n",
    "#client.display_company_filings(cik)\n",
    "#client.display_all_search_results(cik, json_request_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2:\n",
    "\n",
    "# As can be seen from a set of table search results, data obtained directly from a HTML or plain text\n",
    "# table of a SEC filing is not consistently organized and can vary between different companies and time frames\n",
    "# one significant issue in parsing HTML tables contained in SEC filings is the prevelance of \"colspan\" and \"rowspan\"\n",
    "# attributes; the HTML table parser (set forth in the Django codebase) duplicates the cell content across all cells\n",
    "# covered by a colspan or row span. That duplication needs to be addressed here\n",
    "\n",
    "# This cell provides the class used to parse the table search results\n",
    "\n",
    "import itertools\n",
    "\n",
    "class SECFilingTableParser():\n",
    "    \n",
    "    # Primary parse method\n",
    "    def parse(self, data_array):\n",
    "        # Preliminary processing dataframe; see description in method below\n",
    "        df = self.clean_df(pandas.DataFrame(data_array))\n",
    "        \n",
    "        # If the dataframe is only 1 row or 1 column, we're not interested in parsing any further\n",
    "        if len(df.index) == 1 or len(df.columns) == 1:\n",
    "            return df\n",
    "        else:\n",
    "            try:\n",
    "                # This parser assumes the first row in the first column of a table is the first row of data\n",
    "\n",
    "                # Find the first column\n",
    "                first_column = df.iloc[:,0].astype('object')\n",
    "                # Find the first row that contains either letters or numbers in the first column\n",
    "                row_start = first_column.str.contains(\"[A-Za-z0-9]\", case=False, regex=True, na=False).idxmax()\n",
    "                \n",
    "                # If text is contained in position [0] then assume that the table only contains columns (and no index for rows)\n",
    "                if row_start == 0:\n",
    "                    return self.all_columns_table(df, row_start)\n",
    "                # Else, parse as if there is an index for the rows\n",
    "                else:\n",
    "                    return self.accounts_table(df, row_start)\n",
    "            except:\n",
    "                print(\"ERROR\")\n",
    "                print(\"Dataframe:\")\n",
    "                display(pandas.DataFrame(data_array))\n",
    "                print(\"array\")\n",
    "                display(data_array)\n",
    "    \n",
    "    def all_columns_table(self, df, row_start):\n",
    "        # Obtain text in the first row, assume that they are the heading for each column\n",
    "        columns = df.iloc[row_start,:]\n",
    "        # Separate data from headings\n",
    "        df = df.loc[row_start+1:,:]\n",
    "        # Set the new column headings\n",
    "        df.columns = columns\n",
    "        \n",
    "        # More cleanup\n",
    "        df = df.replace('^\\s*$', numpy.nan, regex=True, inplace=False)\n",
    "        # Assumed that there are duplicate columns headings; saving a final set of column names to be used later\n",
    "        final_columns = df.columns.unique().dropna()\n",
    "        \n",
    "        # To elminate the duplicate cell content without losing any other unique data, the data is groupedby\n",
    "        # the columns and then the process_account_name function concatenates all unique data while ignoring\n",
    "        # duplicates\n",
    "        new_df = {}\n",
    "        for i,g in df.groupby(df.columns, axis=1):\n",
    "            new_df[i] = g.apply(lambda x: self.process_account_name(x), axis=1).values.tolist()\n",
    "        new_df = pandas.DataFrame(new_df)\n",
    "        new_df = new_df[final_columns]\n",
    "\n",
    "        # More clean up\n",
    "        new_df.dropna(how=\"all\", axis=1, inplace=True)\n",
    "        new_df.dropna(how=\"all\", axis=0, inplace=True)\n",
    "        return new_df        \n",
    "\n",
    "    def clean_dollar_amounts(self, df):\n",
    "        # This function eliminates unnecessary markings usually found in these tables\n",
    "        df.replace('[$]', \"\", regex=True, inplace=True)\n",
    "        df.replace('[)]', \"\", regex=True, inplace=True)\n",
    "        df.replace('[(]', \"-\", regex=True, inplace=True)\n",
    "        df.replace('[,]', \"\", regex=True, inplace=True)\n",
    "        df = df.apply(lambda x: x.str.strip(), axis=1)\n",
    "        return df\n",
    "                    \n",
    "    def accounts_table(self, df, row_start):\n",
    "        # For these types of tables, assume that first row is the start of the row index\n",
    "        first_row = df.iloc[row_start-1,:].astype('object')\n",
    "        # Assume that the next cell of text in our \"first row\" is the first column of data\n",
    "        col_start = first_row.astype('object').str.contains(\"[A-Za-z0-9]\", case=False, regex=True, na=False).idxmax()\n",
    "\n",
    "        # Assume that all columns before \"col_start\" is a part of the row index\n",
    "        index_columns = df.iloc[row_start:, :col_start]\n",
    "        # Use process_account_name method to eliminate duplicates but concatenate unique text\n",
    "        index = index_columns.apply(lambda x: self.process_account_name(x), axis=1)\n",
    "\n",
    "        # Assume that all rows above \"row_start\" and columns after \"col_start\" pertain to the column headings\n",
    "        column_header_rows = df.iloc[:row_start, col_start:]\n",
    "        # Use process_account name to elminate duplicates and concatenate unique text\n",
    "        columns = column_header_rows.apply(lambda x: self.process_account_name(x))\n",
    "        \n",
    "        # Separate data from row index and column headings\n",
    "        df = df.loc[row_start:, col_start:]\n",
    "        # Eliminate unncessary markings usually found in numeric data\n",
    "        df = self.clean_dollar_amounts(df)\n",
    "        # Clean up spaces\n",
    "        df = df.replace('^\\s*$', numpy.nan, regex=True, inplace=False)\n",
    "        \n",
    "        # Add column headings to data\n",
    "        df.columns = columns\n",
    "        # Add row index to data\n",
    "        df.insert(0, \"account\", index)\n",
    "        # Clean up duplicate or blank columns\n",
    "        final_columns = df.columns.unique().dropna()\n",
    "        blank_columns = final_columns.str.contains(\"^\\s*$\", regex=True, na=True)\n",
    "        final_columns = final_columns[~blank_columns]\n",
    "\n",
    "        # To elminate the duplicate cell content without losing any other unique data, the data is groupedby\n",
    "        # he columns and then the process_account_name function concatenates all unique data while ignoring\n",
    "        # duplicates\n",
    "        new_df = {}\n",
    "        for i,g in df.groupby(df.columns, axis=1):\n",
    "            new_df[i] = g.apply(lambda x: self.process_account_name(x), axis=1).values.tolist()\n",
    "        new_df = pandas.DataFrame(new_df)\n",
    "        new_df = new_df[final_columns]\n",
    "        \n",
    "        # More clean up\n",
    "        new_df.dropna(how=\"all\", axis=1, inplace=True)\n",
    "        new_df.dropna(how=\"all\", axis=0, inplace=True)\n",
    "        return new_df\n",
    "\n",
    "    def process_account_name(self, old_series):\n",
    "        # Method to process related adjacent cells to elminate duplicates but keep unique text\n",
    "        series = old_series.dropna()\n",
    "        return \" \".join([i for i,g in itertools.groupby(series, key=lambda x: x)])\n",
    "    \n",
    "    def clean_df(self, df):\n",
    "        # Preliminary processing of data\n",
    "        new_df = df\n",
    "        # Remove extraneous weird spaces/characters\n",
    "        new_df = new_df.replace(self.extraneous_chars(), \"\", regex=True, inplace=False)\n",
    "        # Replace all blank cells with nan\n",
    "        new_df = new_df.replace('^\\s*$', numpy.nan, regex=True, inplace=False)\n",
    "        # Replace all cells that have the string \"None\" in it\n",
    "        new_df = new_df.replace(\"^\\s*None\\s*$\", numpy.nan, regex=True, inplace=False)\n",
    "        # Replace all cells that have the None type object with nan\n",
    "        new_df = new_df.fillna(numpy.nan)\n",
    "        # Replace words commonly found in the first row in the first column but that is not the\n",
    "        # Start of real data\n",
    "        new_df = new_df.replace(self.blacklist_repeating_headers(), numpy.nan, regex=True, inplace=False)\n",
    "        # Process to eliminate all blank rows and columns\n",
    "        nonempty_rows = new_df.nunique(axis=1, dropna=False) <= 1 #df.apply(lambda x: is_all_same(x), axis=1)\n",
    "        nonempty_columns = new_df.nunique(axis=0, dropna=False) <= 1 #df.apply(lambda x: is_all_same(x))\n",
    "        new_df = new_df.loc[~nonempty_rows, ~nonempty_columns]\n",
    "        # Set new numbered indexes and columns \n",
    "        new_df.index = range(new_df.shape[0])\n",
    "        new_df.columns = range(new_df.shape[1])\n",
    "        return new_df\n",
    "\n",
    "    def blacklist_repeating_headers(self):\n",
    "        return [\n",
    "            \"^\\s*ANALYSIS OF LOSS AND LOSS ADJUST\",\n",
    "            \"^\\s*RECONCILIATION OF NET RESERVES FOR LOSSES\",\n",
    "            \"^\\s*[(].*[Tt][Hh][Oo][Uu][Ss][Aa][Nn][Dd].*[)]\\s*$\",\n",
    "            \"^\\s*[(].*[Mm][Ii][Ll][Ll][Ii][Oo][Nn].*[)]\\s*$\",\n",
    "            \"^\\s*[Yy][Ee][Aa][Rr][.]*\\s*[Ee][Nn][Dd][Ee][Dd].*$\",\n",
    "            \"^\\s*[Ff][Oo][Rr]\\s*[Tt][Hh][Ee]\\s*[Yy][Ee][Aa][Rr][.]*\\s*[Ee][Nn][Dd][Ee][Dd].*$\",\n",
    "            \"^\\s*CONSOLIDATED STATEMENTS OF INCOME\\s*$\",\n",
    "            \"For the years ended\",\n",
    "            \"FOR THE YEARS ENDED\",\n",
    "            \"[(]millio\",\n",
    "            \"ns-except per sh\",\n",
    "            \"are amounts[)]\",\n",
    "            \"^\\s*December 31,\\s*$\",\n",
    "            \"^\\s*ecember 31,\\s*$\",\n",
    "            \"^\\s*DECEMBER 31,\\s*$\",\n",
    "            \"Years Ended December 31\"\n",
    "        ]\n",
    "    \n",
    "    def extraneous_chars(self):\n",
    "        return [\n",
    "            '\\xa0',\n",
    "            '\\xA0',\n",
    "            '\\n',\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Tthis cell can be used to execute the parsing of the table search results\n",
    "\n",
    "# Table search results using parameters set in prior part\n",
    "client = DatabaseAPIClient()\n",
    "tables = client.search_results(cik, json_request_data)\n",
    "\n",
    "parser = SECFilingTableParser()\n",
    "\n",
    "for table in tables:\n",
    "    print(\"*** table prior to parsing ***\")\n",
    "    display(pandas.DataFrame(table))\n",
    "    print(\"*** table after parsing ***\")\n",
    "    display(parser.parse(table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
